{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Scraper\n",
    "Scrapes data from a website and parses it into a csv file. This script is based on the PH tutorial *Intro to Beautiful Soup* by Jeri Wieringa (for getting data from one page) and the BetterProgramming Tutorial *How to Scrape Multiple Pages of a Website Using a Python Web Scraper* by Angelica Dietzel (for getting data from multiple pages). Documentations for *Beautiful Soup* can be found here: https://www.crummy.com/software/BeautifulSoup/bs4/doc/#find-all\n",
    "The website that I'm scraping is Lexikon Japans Studierende provided by the Staatsbibliothe Berlin: https://themen.crossasia.org/japans-studierende/index/show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from requests import get\n",
    "from bs4 import BeautifulSoup\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**re** is not required if you don't use regex to extract text. Python3 works better for this script; Python2 doesn't handle utf-8 too well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import sleep\n",
    "from random import randint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is added to slow down requests rate from the website."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = []\n",
    "places = []\n",
    "years = []\n",
    "texts = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initiates empty containers for stored data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "pages = np.arange(0, 52)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All the entries of \"Lexikon Japans Studierende\" can be found on 52 consecutive webpages. I'm only using **start** and **stop**. The **step** argument does not need to be defined; default = 1 is what I need in this case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "for page in pages:\n",
    "    page = requests.get(\"https://themen.crossasia.org/japans-studierende/index/show/page/\" + str(page))\n",
    "    soup = BeautifulSoup(page.text, features=\"lxml\")\n",
    "    jstudies = soup.find_all('table', width=\"100%\")\n",
    "    \n",
    "    sleep(randint(2,10))\n",
    "    \n",
    "    for container in jstudies:\n",
    "        name= container.find(\"b\", string=re.compile(\"^(?!.*(Name|\\d))\")).get_text()\n",
    "        names.append(name)\n",
    "    \n",
    "        tds = container.find_all(\"td\")\n",
    "    \n",
    "        place=str(tds[3].get_text(strip=True)) if str(tds[3].get_text(strip=True)) else '-'\n",
    "        places.append(place)\n",
    "    \n",
    "        year=str(tds[5].get_text(strip=True)) if str(tds[5].get_text(strip=True)) else '-'\n",
    "        years.append(year)\n",
    "    \n",
    "        text=str(tds[7].get_text(strip=True)) if str(tds[7].get_text(strip=True)) else '-'\n",
    "        texts.append(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defines what **Beautiful Soup** will get as raw data: results from **requests** with the numbers 1-52 appended to the base-url for every subsequent webpage. Parser is lxml."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**jstudies** pre-defines the frame within which I will look for the data. All the data that I want to extract (names, dates, places..) can be found within the individual 'table' tags (specifically those that have width=\"100%\"). All subsequent searches will refer to this **jstudies** container."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**sleep** is pausing requests from website. Random pause between 2-10 seconds."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After \"for container in jstudies:\" comes the actual scraping code. The first expression grabs the names from within the 'b' tag with a regex (excluding \"Name\" and any digitals; which are also in bold font). Places, years, and texts can be found within table data ('td') tags within each 'table'. With \"str\" the data is extracted as string. The \\[\\] identifies the position of the individual 'td'; strip=True is required to get rid of any newlines and tabs (\\t and \\n). The \"if\" clause makes sure the code works with missing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "aus_studies = pd.DataFrame({\n",
    "'person': names,\n",
    "'herkunft': places,\n",
    "'lebensdaten': years,\n",
    "'karriere': texts,\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fills pandas DataFrame with grabbed content from craping code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                     person                    herkunft           lebensdaten  \\\n",
      "0         Abe Bunjirô 阿部文次郎                     Niigata            21.3.1880–   \n",
      "1             Abe Isoo 阿部磯雄                     Fukuoka    4.2.1865–10.2.1949   \n",
      "2        Abe Masayoshi 阿部正義                   Fukui-shi  2.7.1860–19.11. 1909   \n",
      "3              Abe Masayuki                       Ehime            11.5.1853–   \n",
      "4         Abe Mikishi 阿部美樹志       Ichinoseki, Iwate-ken    4.5.1883–20.2.1965   \n",
      "...                     ...                         ...                   ...   \n",
      "2546      Yukawa Genyô 湯川玄洋                Wakayama-shi   15.8.1867–10.8.1935   \n",
      "2547  Yumoto Takehiko 湯本武比古  Shimotakai-gun, Nagano-ken   1.12.1857–27.9.1925   \n",
      "2548   Yunome Suketaka 湯目補隆                      Sendai                     -   \n",
      "2549       Yusa Yoshio 遊佐慶夫               Fukushima-ken            Jan. 1889–   \n",
      "2550     Zushisaki Sukamoto                           -                     -   \n",
      "\n",
      "                                               karriere  \n",
      "0     1912–1914 Recht ♦ SS 1913–WS 1913/14 – U Berli...  \n",
      "1     WS 1894/95 Theologie – U Berlin, Steglitzerstr...  \n",
      "2     1885–1890 und 1896–1898 Ingenieurwesen und Phi...  \n",
      "3     SS 1904–WS 1904/05 Medizin – U Greifswald, Lan...  \n",
      "4     SS 1914 Ingenieurwesen (Tiefbau) – TH Hannover...  \n",
      "...                                                 ...  \n",
      "2546  1900–1902 Medizin (Innere) ♦ WS 1900/01 – U Wü...  \n",
      "2547  GH WS 1890/91 Pädagogik– U Berlin.Pädagoge, Pr...  \n",
      "2548  SS 1893 Recht – U Kiel.Deutschlehrer an der Me...  \n",
      "2549  SS 1914 Recht – U Berlin, Friedenau, Wilhelmst...  \n",
      "2550  SS 1888–WS 1889/90 Recht – U Berlin, NW Schuma...  \n",
      "\n",
      "[2551 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "print(aus_studies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "aus_studies.to_csv('aus_studies_compl.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exports table as csv file."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
